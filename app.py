import streamlit as st
import os
import re
import time
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from groq import Groq

# --- 1. PAGE CONFIG (Must be first) ---
st.set_page_config(
    page_title="DP Assistant",
    page_icon="ğŸ’¬",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- 2. CUSTOM CSS ---
st.markdown("""
    <style>
    #MainMenu, header, footer, .stDeployButton {
        visibility: hidden !important;
        display: none !important;
    }
    
    .stApp { margin: 0 !important; }
    
    .main .block-container {
        padding: 0.5rem 1rem 1rem 1rem !important;
        max-width: 100% !important;
    }
    
    .stChatFloatingInputContainer {
        bottom: 0 !important;
        background: white !important;
        padding: 10px !important;
        border-top: 1px solid #eee;
    }
    
    [data-testid="stChatMessage"]:has([data-testid="stChatMessageAvatarUser"]) {
        flex-direction: row-reverse;
        text-align: right;
        background-color: #002147 !important;
        color: #ffffff !important;
        border-radius: 16px 16px 4px 16px !important;
        margin-left: auto !important;
        margin-right: 0 !important;
        width: fit-content !important;
        max-width: 85%;
        padding: 10px 14px !important;
        margin-bottom: 8px;
    }
    
    [data-testid="stChatMessage"]:has([data-testid="stChatMessageAvatarUser"]) p {
        color: #ffffff !important;
    }
    
    [data-testid="stChatMessageAvatarUser"] { display: none !important; }
    
    [data-testid="stChatMessage"]:has(img) {
        background-color: #f1f3f4 !important;
        color: #1a1a1a !important;
        border-radius: 16px 16px 16px 4px !important;
        width: fit-content !important;
        max-width: 85%;
        padding: 10px 14px !important;
        margin-bottom: 8px;
    }
    
    [data-testid="stChatMessage"] img {
        width: 28px !important;
        height: 28px !important;
    }
    
    .stChatInput { border-radius: 20px !important; }
    .stChatInput > div { border-radius: 20px !important; border: 1px solid #ddd !important; }
    [data-testid="stSidebar"] { display: none !important; }
    
    ::-webkit-scrollbar { width: 6px; }
    ::-webkit-scrollbar-track { background: #f1f1f1; }
    ::-webkit-scrollbar-thumb { background: #c1c1c1; border-radius: 3px; }
    
    .arabic-text { direction: rtl; text-align: right; }
    </style>
""", unsafe_allow_html=True)

# --- 3. LOAD KNOWLEDGE BASE ---
@st.cache_resource
def load_retriever():
    try:
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        vectorstore = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return vectorstore.as_retriever(search_kwargs={"k": 4}), None
    except Exception as e:
        return None, str(e)

retriever, retriever_error = load_retriever()

# --- 4. PATHS ---
logo_path = "data/logo_transparent.png"
if not os.path.exists(logo_path):
    logo_path = None

# --- 5. GROQ CLIENT ---
client = None
api_error = None
try:
    api_key = st.secrets.get("GROQ_API_KEY", None)
    if api_key:
        client = Groq(api_key=api_key)
    else:
        api_error = "GROQ_API_KEY not found in secrets"
except Exception as e:
    api_error = str(e)

# --- 6. MODEL CONFIGURATION ---
GROQ_MODEL = "llama-3.3-70b-versatile"
BACKUP_MODEL = "llama-3.1-8b-instant"  # Faster backup model

# --- 7. GREETINGS ---
GREETING_EN = """Hello! Welcome to **Digital Protection**.

I am here to help you with your questions.

How can I help you?"""

GREETING_AR = """<div class="arabic-text">

Ù…Ø±Ø­Ø¨Ø§! Ø§Ù‡Ù„Ø§ Ø¨Ùƒ ÙÙŠ **Digital Protection**.

Ø§Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ø³Ø¦Ù„ØªÙƒ.

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ

</div>"""

# --- 8. SYSTEM INSTRUCTIONS ---
SYSTEM_INSTRUCTIONS_EN = """You are DP Assistant for Digital Protection, a data protection consultancy in Amman, Jordan.

LANGUAGE: Respond in ENGLISH only.

RULES:
1. NO EMOJIS ever
2. NO LEGAL ADVICE - say "I cannot provide legal advice. Please consult a qualified legal professional."
3. NO CONTRACTS - say "I cannot generate contracts. Please contact our team."
4. NO SPECIFIC PRICES - say pricing depends on scope
5. NO IT SUPPORT for printers, WiFi, hardware

STYLE: Keep responses SHORT (2-4 sentences). Professional but friendly.

SERVICES:
- Privacy & Compliance: GDPR, ISO 27701, CBJ
- Security Assessments: Vulnerability scanning, risk analysis
- Network Security: Firewalls, WAF
- Identity & Access Management: IAM/PAM

CONTACT: info@dp-technologies.net | +962 790 552 879 | Amman, Jordan"""

SYSTEM_INSTRUCTIONS_AR = """Ø§Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ DP Ù„Ø´Ø±ÙƒØ© Digital Protection ÙÙŠ Ø¹Ù…Ø§Ù†ØŒ Ø§Ù„Ø§Ø±Ø¯Ù†.

Ø§Ù„Ù„ØºØ©: Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
1. Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ø¨Ø¯Ø§
2. Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© - Ù‚Ù„ "Ù„Ø§ Ø§Ø³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…Ø­Ø§Ù… Ù…Ø®ØªØµ."
3. Ø¨Ø¯ÙˆÙ† Ø¹Ù‚ÙˆØ¯ - Ù‚Ù„ "Ù„Ø§ Ø§Ø³ØªØ·ÙŠØ¹ Ø§Ù†Ø´Ø§Ø¡ Ø¹Ù‚ÙˆØ¯. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚Ù†Ø§."
4. Ø¨Ø¯ÙˆÙ† Ø§Ø³Ø¹Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© - Ù‚Ù„ Ø§Ù„ØªØ³Ø¹ÙŠØ± ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
5. Ù…Ù…Ù†ÙˆØ¹ Ù…Ù†Ø¹Ø§ Ø¨Ø§ØªØ§ ØªÙ‚Ø¯ÙŠÙ… Ø¯Ø¹Ù… ØªÙ‚Ù†ÙŠ Ù„Ù„Ø·Ø§Ø¨Ø¹Ø§Øª (Printers) Ø§Ùˆ Ø§Ù„ÙˆØ§ÙŠ ÙØ§ÙŠ (WiFi) Ø§Ùˆ Ø§Ù„Ø§Ø¬Ù‡Ø²Ø©. Ù‚Ù„ "Ø¹Ø°Ø±Ø§ØŒ Ù‡Ø°Ø§ Ø®Ø§Ø±Ø¬ Ù†Ø·Ø§Ù‚ Ø®Ø¯Ù…Ø§ØªÙ†Ø§."

Ø§Ù„Ø§Ø³Ù„ÙˆØ¨: Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© (2-4 Ø¬Ù…Ù„). Ù…Ù‡Ù†ÙŠ ÙˆÙˆØ¯ÙˆØ¯.

Ø§Ù„Ø®Ø¯Ù…Ø§Øª:
- Ø§Ù„Ø®ØµÙˆØµÙŠØ© ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„: GDPRØŒ ISO 27701ØŒ Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ø§Ù„Ø§Ø±Ø¯Ù†ÙŠ
- ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ø§Ù…Ù†: ÙØ­Øµ Ø§Ù„Ø«ØºØ±Ø§ØªØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
- Ø§Ù…Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª: Ø¬Ø¯Ø±Ø§Ù† Ø§Ù„Ø­Ù…Ø§ÙŠØ©ØŒ WAF
- Ø§Ø¯Ø§Ø±Ø© Ø§Ù„Ù‡ÙˆÙŠØ© ÙˆØ§Ù„ÙˆØµÙˆÙ„: IAM/PAM

Ø§Ù„ØªÙˆØ§ØµÙ„: info@dp-technologies.net | +962 790 552 879 | Ø¹Ù…Ø§Ù†ØŒ Ø§Ù„Ø§Ø±Ø¯Ù†"""

# --- 9. HELPER FUNCTIONS ---
# Compile regex patterns once globally
ARABIC_PATTERN = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]+')
EMOJI_PATTERN = re.compile("["
    u"\U0001F600-\U0001F64F"
    u"\U0001F300-\U0001F5FF"
    u"\U0001F680-\U0001F6FF"
    u"\U0001F1E0-\U0001F1FF"
    u"\U00002702-\U000027B0"
    u"\U000024C2-\U0001F251"
    "]+", flags=re.UNICODE)
LABELS_TO_REMOVE = ["Direct answer:", "Key Points:", "Key Considerations:", "Next Step:", 
          "Response:", "Answer:", "Ø§Ù„Ø§Ø¬Ø§Ø¨Ø©:", "Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:", "Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:"]

def is_arabic(text):
    return bool(ARABIC_PATTERN.search(text))

def clean_response(answer, is_arabic_response=False):
    """Clean up the response text"""
    # Remove robotic labels
    for label in LABELS_TO_REMOVE:
        if label in answer:
            answer = answer.replace(label, "")
    
    # Remove emojis
    answer = EMOJI_PATTERN.sub('', answer)
    
    # Clean whitespace
    if "\n\n\n" in answer:
        answer = answer.replace("\n\n\n", "\n\n")
    
    answer = answer.strip()
    
    # Wrap Arabic in RTL div
    if is_arabic_response:
        answer = f'<div class="arabic-text">{answer}</div>'
    
    return answer

def get_fallback_response(prompt, is_arabic_lang):
    """Get a fallback response when API fails"""
    prompt_lower = prompt.lower()
    fallback = FALLBACK_AR if is_arabic_lang else FALLBACK_EN
    
    if any(word in prompt_lower for word in ["service", "Ø®Ø¯Ù…", "offer", "ØªÙ‚Ø¯Ù…"]):
        return fallback["services"]
    elif any(word in prompt_lower for word in ["price", "cost", "Ø³Ø¹Ø±", "ØªÙƒÙ„Ù", "ÙƒÙ…"]):
        return fallback["pricing"]
    elif any(word in prompt_lower for word in ["where", "location", "Ø§ÙŠÙ†", "Ù…ÙˆÙ‚Ø¹"]):
        return fallback["location"]
    else:
        return fallback["default"]

# --- 10. FALLBACK RESPONSES ---
FALLBACK_EN = {
    "services": "We offer cybersecurity and compliance services including GDPR, ISO 27701, CBJ compliance, security assessments, and identity management. Contact us at info@dp-technologies.net for details.",
    "pricing": "Pricing depends on the scope of your project. We offer fixed-price, time and materials, and retainer options. Contact info@dp-technologies.net for a quote.",
    "location": "We are located in Amman, Jordan. Contact us at info@dp-technologies.net or +962 790 552 879.",
    "default": "Thank you for your message. For detailed assistance, please contact our team at info@dp-technologies.net or +962 790 552 879."
}

FALLBACK_AR = {
    "services": "Ù†Ù‚Ø¯Ù… Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø§Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ GDPR Ùˆ ISO 27701 ÙˆØ§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ø§Ù„Ø§Ø±Ø¯Ù†ÙŠ ÙˆØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ø§Ù…Ù†. ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ Ø¹Ù„Ù‰ info@dp-technologies.net",
    "pricing": "Ø§Ù„ØªØ³Ø¹ÙŠØ± ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ Ù…Ø´Ø±ÙˆØ¹Ùƒ. Ù†Ù‚Ø¯Ù… Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø«Ø§Ø¨Øª ÙˆØ§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø§Ø´ØªØ±Ø§Ùƒ. ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø±Ø¶ Ø³Ø¹Ø±.",
    "location": "Ù†Ø­Ù† ÙÙŠ Ø¹Ù…Ø§Ù†ØŒ Ø§Ù„Ø§Ø±Ø¯Ù†. ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ Ø¹Ù„Ù‰ info@dp-technologies.net Ø§Ùˆ +962 790 552 879",
    "default": "Ø´ÙƒØ±Ø§ Ù„Ø±Ø³Ø§Ù„ØªÙƒ. Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚Ù†Ø§ Ø¹Ù„Ù‰ info@dp-technologies.net Ø§Ùˆ +962 790 552 879"
}

# --- 11. INITIALIZE SESSION STATE ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "ui_language" not in st.session_state:
    st.session_state.ui_language = "en"

if "greeting_shown" not in st.session_state:
    st.session_state.greeting_shown = False

if "error_count" not in st.session_state:
    st.session_state.error_count = 0

# --- 12. HEADER WITH LANGUAGE TOGGLE ---
query_params = st.query_params
is_embedded = query_params.get("embed", "false").lower() == "true"

if not is_embedded:
    col1, col2, col3 = st.columns([1, 4, 2])
    with col1:
        if logo_path:
            st.image(logo_path, width=50)
    with col2:
        st.markdown("### Digital Protection Support")
    with col3:
        if st.session_state.ui_language == "en":
            if st.button("Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", key="lang_toggle"):
                st.session_state.ui_language = "ar"
                st.session_state.messages = []
                st.session_state.greeting_shown = False
                st.rerun()
        else:
            if st.button("English", key="lang_toggle"):
                st.session_state.ui_language = "en"
                st.session_state.messages = []
                st.session_state.greeting_shown = False
                st.rerun()
else:
    col1, col2 = st.columns([5, 1])
    with col2:
        if st.session_state.ui_language == "en":
            if st.button("Ø¹Ø±Ø¨ÙŠ", key="lang_toggle_embed"):
                st.session_state.ui_language = "ar"
                st.session_state.messages = []
                st.session_state.greeting_shown = False
                st.rerun()
        else:
            if st.button("EN", key="lang_toggle_embed"):
                st.session_state.ui_language = "en"
                st.session_state.messages = []
                st.session_state.greeting_shown = False
                st.rerun()

# --- 12.5 ERROR NOTIFICATIONS ---
if retriever_error:
    st.error(f"âš ï¸ Knowledge Base Error: {retriever_error}. The bot will answer without context.")
if api_error:
    st.error(f"âš ï¸ API Error: {api_error}. Please check your API key.")

# --- 13. SHOW GREETING ---
if not st.session_state.greeting_shown:
    if st.session_state.ui_language == "ar":
        st.session_state.messages = [{"role": "assistant", "content": GREETING_AR}]
    else:
        st.session_state.messages = [{"role": "assistant", "content": GREETING_EN}]
    st.session_state.greeting_shown = True

# --- 14. DISPLAY CHAT HISTORY ---
for msg in st.session_state.messages:
    avatar = logo_path if msg["role"] == "assistant" else None
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"], unsafe_allow_html=True)

# --- 15. CHAT INPUT (ROBUST VERSION) ---
input_placeholder = "Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ..." if st.session_state.ui_language == "ar" else "Type your message..."

if prompt := st.chat_input(input_placeholder):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant", avatar=logo_path):
        response_placeholder = st.empty()
        
        # 1. Search knowledge base
        context = ""
        if retriever:
            try:
                search_results = retriever.invoke(prompt)
                context = "\n".join([doc.page_content for doc in search_results])
                print(f"DEBUG: Found {len(search_results)} chunks for query.")
            except Exception as e:
                print(f"DEBUG: Retriever failed during invoke: {e}")
        
        # 2. Check language
        user_is_ar = is_arabic(prompt) or st.session_state.ui_language == "ar"
        system_prompt = SYSTEM_INSTRUCTIONS_AR if user_is_ar else SYSTEM_INSTRUCTIONS_EN

        # 3. Call API with Fallback Logic
        stream = None
        used_model = GROQ_MODEL
        
        try:
            # Try Primary Model (70b)
            stream = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": f"{system_prompt}\n\nUSE THIS CONTEXT TO ANSWER:\n{context}"},
                    {"role": "user", "content": prompt}
                ],
                model=GROQ_MODEL,
                stream=True,
            )
        except Exception as e:
            print(f"Primary model failed: {e}")
            # If rate limited or other error, try Backup Model (8b)
            try:
                if "rate" in str(e).lower() and "limit" in str(e).lower():
                     error_msg = "Daily limit reached for smart model. Switching to standard model."
                else:
                     error_msg = "Smart model unavailable. Switching to standard model."
                
                print(f"Switching to backup: {error_msg}")
                st.toast(f"âš ï¸ {error_msg}", icon="âš ï¸")
                
                stream = client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": f"{system_prompt}\n\nUSE THIS CONTEXT TO ANSWER:\n{context}"},
                        {"role": "user", "content": prompt}
                    ],
                    model=BACKUP_MODEL,
                    stream=True,
                )
                used_model = BACKUP_MODEL
            except Exception as e2:
                print(f"Backup model also failed: {e2}")
                stream = None

        # 4. Process Stream or Show Static Fallback
        if stream:
            try:
                full_response = ""
                last_update_time = time.time()
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        current_time = time.time()
                        if current_time - last_update_time > 0.05:
                            display_text = clean_response(full_response, user_is_ar)
                            response_placeholder.markdown(display_text + "â–Œ", unsafe_allow_html=True)
                            last_update_time = current_time
                
                final_answer = clean_response(full_response, user_is_ar)
                
                # Append disclaimer if using backup model
                # if used_model == BACKUP_MODEL:
                #      final_answer += "\n\n_Note: Using standard model due to high traffic._"
                
                response_placeholder.markdown(final_answer, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": final_answer})
            
            except Exception as e:
                 print(f"Stream processing error: {e}")
                 fallback = get_fallback_response(prompt, user_is_ar)
                 response_placeholder.markdown(fallback)
        else:
            # If both models failed
            fallback = get_fallback_response(prompt, user_is_ar)
            response_placeholder.markdown(fallback)
